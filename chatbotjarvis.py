# -*- coding: utf-8 -*-
"""ChatbotJarvis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C2YZz7pBL0fRkF5N8NWuVSdx1xfDQz8m
"""

import nltk
nltk.download('punkt')
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

import tensorflow as tf
import numpy as np
import random
import json

from google.colab import files
files.upload()

with open ('intents3.json') as json_data:
  intents3 = json.load(json_data)

intents3

words = []
classes = []
documents = []
ignore = ['?']
for intent in intents['intents']:
  for pattern in intent['patterns']:
    w = nltk.word_tokenize(pattern)
    words.extend(w)
    documents.append((w, intent['tag']))
    if intent['tag'] not in classes:
      classes.append(intent['tag'])

words = [stemmer.stem(w.lower()) for w in words if w not in ignore]
words = sorted(list(set(words)))
classes = sorted(list(set(classes)))
print (len(documents), "documents")
print (len(classes), "classes", classes)
print (len(words), "unique stemmed words", words)

training = []
output = []
output_empty = [0] * len(classes)
for doc in documents:
  bag = []
  pattern_words = doc[0]
  pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]
  for w in words:
    bag.append(1) if w in pattern_words else bag.append(0)
    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1
    training.append([bag, output_row])
random.shuffle(training)
training = np.array(training)
train_x = list(training[:,0])
train_y = list(training[:,1])

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(10,input_shape=(len(train_x[0]),)))
model.add(tf.keras.layers.Dense(10))
model.add(tf.keras.layers.Dense(len(train_y[0]), activation='softmax'))
model.compile(tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=8, verbose=1)
model.save("model.pkl")

import pickle
pickle.dump( {'words':words, 'classes':classes}, open( "training_data", "wb" ) )

from keras.models import load_model
model = load_model("model.pkl")

def clean_up_sentence(sentence):
  sentence_words = nltk.word_tokenize(sentence)
  sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]
  return sentence_words
def bow(sentence, words):
  sentence_words = clean_up_sentence (sentence)
  bag = [0]*len(words)
  for s in sentence_words:
    for i,w in enumerate(words):
      if w ==s:
        bag[i] = 1
  bag=np.array(bag)
  return(bag)

ERROR_THRESHOLD = 0.30
def classify(sentence): 
  bag = bow(sentence, words)
  results = model.predict(np.array([bag]))
  results = [[i,r] for i,r in enumerate(results[0]) if r>ERROR_THRESHOLD]
  results.sort(key=lambda x: x[1], reverse=True)
  return_list = []
  for r in results:
    return_list.append((classes[r[0]], r[1]))
    return return_list
def response(sentence):
  results = classify(sentence)
  if results:
    while results:
      for i in intents['intents']:
        if i['tag'] == results[0][0]: 
             return print(random.choice(i["responses"]))
      results.pop(0)

response('Where are you located?')

response('Bye')

